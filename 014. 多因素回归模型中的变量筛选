多元线性回归、logistic回归和Cox回归是医学统计分析中使用最多的三种回归方法，
我们可以借助统计软件的变量筛选方法自动实现变量筛选，
现实情况是，我们在读临床文献的时候，很多作者采用下面一种变量筛选的方法：
首先逐个对变量进行单因素回归分析，把单因素回归分析P值小于0.1的纳入最终的回归方程
（此处变量筛选的标准也可把P值设为0.05或0.2，一般不会设置小于0.05，也不会设置大于0.2）。
这两种方法到底该如何选择呢？
坦率的讲，这个问题没有标准答案。
但变量筛选应该考虑以下几条基本原则：

第一种情况，当有效样本量很大，统计学检验效能足够的时候，
可以使用变量自动筛选的方法中的任何一种。
这里有一个经验性的判断统计学效能是否足够的标准：
即一个单变量因素至少有20个有效样本量，
举例来说，比如我们做Cox回归分析，
如果我们收集了10个与预后相关的变量，那么至少应该有200个患者出现了我们定义的终点事件，比如死亡，
此处需要注意的是至少200个死亡患者，而非200个患者，
未出现终点事件的样本我们一般不把其视为有效样本。

第二种情况，当不满足上述条件，或者其他原因导致的统计学效能不够的情况，
应该采用大多数临床研究报告中采用的变量筛选方法，
即首先逐个对变量进行单因素回归分析，把单因素回归分析P值小于0.1的纳入最终的回归方程。
这种方法虽然广泛使用，但也饱受统计学家的质疑。

第三种情况，即便是第二种方法，也未必可以“高枕无忧”了，
有时我们会发现某些确定与某种疾病临床预后相关的变量，在单因素分析的时候并未达到我们所设定的变量筛选标准，而被排除在多因素回归模型之外，
比如在一个前列腺癌预后因素分析的研究中，
作者并未发现Gleason评分与预后显著相关，而临床上比较肯定的是Gleason评分与前列腺癌患者的预后显著相关，
此时我们应该怎样做出取舍呢？
对于那些已知的确定与某疾病预后显著相关的变量，即便未达到我们设定的统计学筛选标准，我们也应该纳入回归模型，
这么做的考量即是从临床专业角度筛选变量。

综上，推荐第三种变量筛选的方法，
统筹考虑统计学上的单因素分析结果与已知临床专业知识决定纳入回归方程的变量。
我们以生存资料为例演示了Cox回归中变量筛选的方法，多因素Logistic回归与多元线性回归的的变量筛选方法与上述Cox回归方法相同。
在上述操作过程中，我们并未从临床专业角度考虑变量的取舍，
众所周知ER与乳腺癌患者的预后相关，但本例中单因素Cox回归分析中变量ER的P值为0.154，并未达到我们设定的筛选标准，并未进入最终的回归模型，
这种做法是否妥当？
正如前文所述，我们也可以兼顾临床专业考虑与统计学考量决定最终纳入回归模型的变量，
即便不符合我们设定的变量筛选标准，也将其纳入最终的回归模型进项校正。
当然作者也可以尝试按照不同的变量筛选原则构建多个回归模型，
通过回归模型诊断、预测效能评价等统计学方法比较不同回归模型的优劣，
比如计算不同回归模型的C-Index或者C-Statistic等。

有关变量筛选的争论从来没停止过，统计学家有统计学家的考量，但临床论文的作者往往并未严格遵照统计学家的建议。
我们暂且搁置争议，先不论对错，目前确实也很难分出对错，
因为临床研究往往有很多现实的困境：
比如样本量不够，比如很难根据我们已知的知识来确定哪些因素才是导致结局发生的“罪魁祸首”。
尽管如此，变量筛选也并非毫无章法可言，回顾顶级医学杂志发表的文章，其中有关变量筛选的方法大体考虑以下几点：

1. 临床专业知识，这一点应该是变量筛选最基础的考量，医学统计分析如果脱离临床可能会变得虚无缥缈。
根据目前临床专业知识，已知的确定与结局发生有关的变量应该纳入回归模型，而不去过多考虑其统计学参数。
如前文所述例子：Gleason评分与前列腺癌预后显著相关，这是我们的共识，
那么对于评价前列腺癌预后影响因素的分析，像Gleason评分这样的变量应该参与建模，而无需去考虑其统计学参数是否有统计学意义。

2. 根据单因素分析结果筛选变量，单因素分析P值“显著”的变量放入多元回归方程。
所谓P值“显著”一般设为P<0.1，也可设为P<0.2或者P<0.05，
需根据样本量大小做出调整，样本量够大可以把P值调小，样本量不足则需要更保守一点，把P值设大。
这一类变量筛选方法在既往发表的临床研究论文中很常见，即便是高分论文中也很常见。
尽管对于这种方法，绝大多数统计学家提出质疑。
而现实情况是：如果弃用这种方法，目前是否有更加准确的、更科学的替代方法呢？
答案显然是否定的，统计学家也并未找到更有说服力的新方法。
我们习惯于轻易地否定一个既有的方案，但在没有找到新的解决方案之前，否定既有的方案是无意义的。

3. 根据混杂因素“Z”对试验因素或暴露因素“X”的影响大小筛选变量。
具体说来，先观察调整“Z”与不调整“Z”，“X”对因变量“Y”的作用是否有变化。
先运行仅纳入“X”的基本模型，记录回归系数β1，再在该模型中加入“Z”，看β1变化多大，通常认为β1变化超过10%则需要调整该变量，否则不需要。
这种方法与第2种根据单因素分析结果筛选变量的差别在于：这里把混杂因素对试验因素的影响量化。
这种方法也并非是完美的，“Z”和“X”对“Y”的影响也同样可能受到其他混杂因素的影响。
如果沿着这个思路继续思考下去，我们会陷入一个思维的“怪圈”。
剩下的更复杂的方法学难题留给更有智慧的人，我们暂且把这个方法认为是一个可供参考的变量筛选方法，
尤其是对于那些研究目的非常明确：明确探究“X”对因变量“Y”的作用，而这种作用很大可能是客观存在的，我们要做的无非是去调整这些混杂因素。

4. 考虑样本量大小决定最终纳入模型的变量个数。
这是一个现实的问题，如果样本量足够大，统计效能足够，我们可以借助统计软件提供的变量筛选方法自动筛选变量，这是一种让研究者赏心悦目的神操作，
主流统计软件提供的变量筛选方法有六种：
① 条件参数估计似然比检验（向前：条件）；
② 最大偏似然估计的似然比检验（向前：LR）；
③ Wald卡方检验（向前：Wald）；
④ 条件参数估计似然比检验（向后：条件）；
⑤ 最大偏似然估计的似然比检验（向后：LR）；
⑥ Wald卡方检验（向后：Wald）。
在统计效能足够的前提下，这六种方法筛选的结果大致相同，可以筛选出统计软件认为合适的独立影响结果的变量。
而很多时候我们往往需要考虑的变量很多，而样本量“捉襟见肘”。
在统计效能和变量筛选之间我们得做出妥协，而妥协才是最大生存智慧。

以上列举了四种变量筛选方法，此外还有很多其他变量筛选方法，
比如根据模型参数：决定系数R^2，AIC，似然比对数、C-Statistics等等。
筛选变量的方法越多，越证明没有最好的方法。
这里也不讨论孰优孰劣，只有基于客观条件的最合适的选择才是好的，所以从这个意义上讲选择合适的变量筛选方法即可。

相较于轻易否定一个方案，我们更需要一个新的科学的解决办法。
那是否有更好的解决办法呢？
根据既往发表的临床研究文献，主要是一些高分文献，
常常采用的方法是：纳入不同的变量，构建多个模型（mode1，model 2，model 3......），客观报告每个模型的结果。
这其实是一种敏感性分析方法，根据不同的变量建立多个模型，在这些不同模型中有些变量始终稳定地影响结果，那么这些变量往往是最接近真相的。
这也是一种“曲线救国”的方案，我们不去争论变量筛选的对与错，而是从结果出发反向寻找影响结局发生的最稳定因素，这是一种智慧！

