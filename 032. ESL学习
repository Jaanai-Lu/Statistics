"""
The Elements of Statistical Learning
"""

在监督学习中，目标是根据一系列输入度量来预测输出度量的值。而在非监督学习中，没有输出度量，它的目标是描述一系列输入度量之间的联系。
之所以称之为监督学习是因为存在结果变量来引导学习的过程。
在非监督学习中，我们仅仅观察特征而没有对结果的测量，我们的任务是描述数据是如何形成的或者成簇的。

DNA表达微阵列通过测量一个细胞某基因的mRNA的量来衡量基因的表达情况。
DNA表达微阵列被当作一个生物学领域的重大突破，促进了对单个细胞内成千上万基因同时量化研究．
一个基因表达数据集收集了一系列DNA微阵列实验中的表达的值，每一列表示一次实验。因此几千行表示几千个不同的基因，每一列表示一个样本：
在图 1.3 中有 6830 个基因（行）和 64 个样本（列），尽管为了清晰只有一个随机样本的100行显示出来。
样本是从不同病人得到的64个癌症肿瘤。
这里的挑战是理解基因和样本是怎样组织起来的．典型的问题有如下几个：
(a) 根据基因的表达谱，哪些样本两两之间最相似？
(b) 根据样本的表达谱，哪些基因两两之间最相似？
(C) 对于特定的癌症样本，特定的基因是否有非常高或低的表达？
我们可以把这个任务看成是回归问题，有两个类别型自变量——基因和样本——以及对应变量的表达水平。
然而，把这个问题看成是非监督学习可能更有用。例如，对于上述的问题(a)，我们把样本看成是6830维空间中的一点，并且想通过某种方式对其进行聚类。

当我们预测定量的输出时被称为 回归 (regression)，当我们预测定性的输出时被称为 分类(classification)。
我们将会看到这两个任务有很多的共同点，特别地，两者都可以看成是函数逼近。

第三种变量类型是 有序分类 (ordered categorical)，如 小(small)、中 (medium) 和 大 (large)，
在这些值之间存在顺序，但是没有合适的度量概念（中与小之间的差异不必和大与中间的差异相等）。

当存在超过两个的类别，最有用并且最普遍使用的编码是 虚拟变量(dummy variables)。
这里有 K 个水平的定性变量被一个 K 位的二进制变量表示，每次只有一个在开启状态。
尽管更简洁的编码模式也是可能的，但虚拟变量在因子的层次中是对称的。

所有的向量都假定为列向量。

线性模型对结构做出很大的假设而得出稳定但可能不正确的预测。k-最近邻方法对结构的假设很温和：它的预测通常是准确的但不稳定。

用向量内积形式写出线性模型。

二元正态分布（不相关且均值不同）？
混合的高斯分布？
双变量高斯分布？

最小二乘法的线性判别边界非常光滑，而且显然拟合得很稳定。它似乎严重依赖线性判别边界是合理的假设，它有低方差和潜在的高偏差。
另一方面，k-最近邻过程似乎不非常依赖任何关于数据的假设，而且可以适用于任何情形。
然而，判别边界的任何特定的分区都依赖几个输入点和它们的特定位置，而且因此左右摇摆不稳定——高方差和低偏差。

今天最受欢迎的技巧的一大部分是这两种方法的变形．事实上，1-最近邻，所有方法中最简单的，夺取了低维问题的大部分市场．
下面列举描述了这些简单的过程增强的方式：
1. 核方法用光滑递减至 0 的权重及距离目标点的距离，而不是像在k-最近邻中用的有效的 0/1 权重系数
2. 高维空间中，距离核修改成比其它的变量更多地强调某些变量
3. 通过局部权重的最小二乘的局部线性回归，而不是局部用常数值来拟合
4. 线性模型适应基本的允许任意复杂模型的原始输入的拓展
5. 投影寻踪和神经网络模型包括非线性模型转换为线性模型之和

损失函数用来惩罚预测中的错误，到目前为止最常用并且最方便的是 均方误差损失。
这促使我们寻找模型评价的一个准则————预测（平方）误差的期望，即Mse准则。




























