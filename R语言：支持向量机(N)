支持向量机是做什么的呢？
支持向量机（SVM）获取一个超平面将数据分成两类。以高水准要求来看，除了不会使用决策树以外，SVM与C4.5算法是执行相似的任务的。
超平面（hyperplane）是个函数，类似于解析一条线的方程。
实际上，对于只有两个属性的简单分类任务来说，超平面可以是一条线的。
其实事实证明：
SVM可以使用一个小技巧，把你的数据提升到更高的维度去处理。一旦提升到更高的维度中，SVM算法会计算出把你的数据分离成两类的最好的超平面。
有例子么？当然，举个最简单的例子。我发现桌子上开始就有一堆红球和蓝球，如果这这些球没有过分的混合在一起，不用移动这些球，你可以拿一根棍子把它们分离开。
你看，当在桌上加一个新球时，通过已经知道的棍子的哪一边是哪个颜色的球，你就可以预测这个新球的颜色了。
最酷的部分是什么呢？SVM算法可以算出这个超平面的方程。
如果事情变得更复杂该怎么办？当然了，事情通常都很复杂。如果球是混合在一起的，一根直棍就不能解决问题了。
下面是解决方案：
快速提起桌子，把所有的球抛向空中，当所有的球以正确的方式抛在空中时，你使用一张很大的纸在空中分开这些球。
你可能会想这是不是犯规了。不，提起桌子就等同于把你的数据映射到了高维空间中。
在这个例子中，我们从桌子表面的二维空间过渡到了球在空中的三维空间。
那么SVM该怎么做呢？
通过使用核函数（kernel），我们在高维空间也有很棒的操作方法。
这张大纸依然叫做超平面，但是现在它对应的方程是描述一个平面而不是一条线了。
根据Yuval的说法，一旦我们在三维空间处理问题，超平面肯定是一个面而不是线了。
那么在桌上或者空中的球怎么用现实的数据解释呢？
桌上的每个球都有自己的位置，我们可以用坐标来表示。
打个比方，一个球可能是距离桌子左边缘20cm 距离底部边缘50cm，另一种描述这个球的方式是使用坐标(x,y)或者(20,50)表达。x和y是代表球的两个维度。
可以这样理解：如果我们有个病人的数据集，每个病人可以用很多指标来描述，比如脉搏，胆固醇水平，血压等。每个指标都代表一个维度。
基本上，SVM把数据映射到一个更高维的空间然后找到一个能分类的超平面。
类间间隔(margin)经常会和SVM联系起来，类间间隔是什么呢？它是超平面和各自类中离超平面最近的数据点间的距离。
在球和桌面的例子中，棍子和最近的红球和蓝球间的距离就是类间间隔(margin)。
SVM的关键在于，它试图最大化这个类间间隔，使分类的超平面远离红球和蓝球。这样就能降低误分类的可能性。
那么支持向量机的名字是哪里来的？
还是球和桌子的例子中，超平面到红球和蓝球的距离是相等的。这些球或者说数据点叫做支持向量，因为它们都是支持这个超平面的。
那这是监督算法还是非监督的呢？SVM属于监督学习。因为开始需要使用一个数据集让SVM学习这些数据中的类型。只有这样之后SVM才有能力对新数据进行分类。
为什么我们要用SVM呢？ 
SVM和C4.5大体上都是优先尝试的二类分类器。根据“没有免费午餐原理”，没有哪一种分类器在所有情况下都是最好的。此外，核函数的选择和可解释性是算法的弱点所在。
在哪里使用 SVM？有什么SVM的实现方法？
比较流行的是用scikit-learn, MATLAB和libsvm实现。

本文讨论了如何在逻辑回归、决策树和SVM之间做出最佳选择。
首先应该选择逻辑回归，然后试试决策树（随机森林）是否可以大幅度提升模型性能；特征的数量和观测样本特别多、资源和时间充足时，可使用SVM。

事实上，这三个算法在其设计之初就赋予了一定的内部特性，我们将其分析透彻的主要目的在于：当你面临商业问题时，这些算法的特性可以让你在选择这些算法时得到一些灵感。

首先，我们来分析下逻辑回归（Logistic Regression）,它是解决工业规模问题最流行的算法，尽管与其他技术相比，其在效率和算法实现的易用性方面并不出众。

逻辑回归非常便利并且很有用的一点就是，它输出的结果并不是一个离散值或者确切的类别。相反，你得到的是一个与每个观测样本相关的概率列表。你可以使用不同的标准和常用的性能指标来分析这个概率分数，并得到一个阈值，然后使用最符合你业务问题的方式进行分类输出。在金融行业，这种技术普遍应用于记分卡中，对于同一个模型，你可以调整你的阈值【临界值】来得到不同的分类结果。很少有其它算法使用这种分数作为直接结果。相反，它们的输出是严谨的直接分类结果。同时，逻辑回归在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。

除此之外，逻辑回归算法对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。严重的多重共线性则可以使用逻辑回归结合L2正则化来解决，不过如果要得到一个简约模型，L2正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。

当你的特征数目很大并且还丢失了大部分数据时，逻辑回归就会表现得力不从心。同时，太多的类别变量对逻辑回归来说也是一个问题。逻辑回归的另一个争议点是它使用整个数据来得到它的概率分数。虽然这并不是一个问题，但是当你尝试画一条分离曲线的时候，逻辑回归可能会认为那些位于分数两端“明显的”数据点不应该被关注。有些人可能认为，在理想情况下，逻辑回归应该依赖这些边界点。同时，如果某些特征是非线性的，那么你必须依靠转换，然而当你特征空间的维数增加时，这也会变成另一个难题。所以，对于逻辑回归，我们根据讨论的内容总结了一些突出的优点和缺点。

逻辑回归的优点：



便利的观测样本概率分数； 
已有工具的高效实现；
对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决；
逻辑回归广泛的应用于工业问题上（这一点很重要）。


逻辑回归的缺点：



当特征空间很大时，逻辑回归的性能不是很好；
不能很好地处理大量多类特征或变量；
对于非线性特征，需要进行转换；
依赖于全部的数据（个人觉得这并不是一个很严重的缺点）。




下面让我们来讨论下决策树和支持向量机。

决策树固有的特性是它对单向变换或非线性特征并不关心[这不同于预测器当中的非线性相关性>，因为它们简单地在特征空间中插入矩形[或是（超）长方体]，这些形状可以适应任何单调变换。当决策树被设计用来处理预测器的离散数据或是类别时，任何数量的分类变量对决策树来说都不是真正的问题。使用决策树训练得到的模型相当直观，在业务上也非常容易解释。决策树并不是以概率分数作为直接结果，但是你可以使用类概率反过来分配给终端节点。这也就让我们看到了与决策树相关的最大问题，即它们属于高度偏见型模型。你可以在训练集上构建决策树模型，而且其在训练集上的结果可能优于其它算法，但你的测试集最终会证明它是一个差的预测器。你必须对树进行剪枝，同时结合交叉验证才能得到一个没有过拟合的决策树模型。

随机森林在很大程度上克服了过拟合这一缺陷，其本身并没有什么特别之处，但它却是决策树一个非常优秀的扩展。随机森林同时也剥夺了商业规则的易解释性，因为现在你有上千棵这样的树，而且它们使用的多数投票规则会使得模型变得更加复杂。同时，决策树变量之间也存在相互作用，如果你的大多数变量之间没有相互作用关系或者非常弱，那么会使得结果非常低效。此外，这种设计也使得它们更不易受多重共线性的影响。

决策树总结如下：

决策树的优点：

直观的决策规则
可以处理非线性特征
考虑了变量之间的相互作用


决策树的缺点：

训练集上的效果高度优于测试集，即过拟合[随机森林克服了此缺点]
没有将排名分数作为直接结果




现在来讨论下支持向量机（SVM, Support Vector Machine）。支持向量机的特点是它依靠边界样本来建立需要的分离曲线。正如我们 之间看到的那样，它可以处理非线性决策边界。对边界的依赖，也使得它们有能力处理缺失数据中“明显的”样本实例。支持向量机能够处理大的特征空间，也因此成为文本分析中最受欢迎的算法之一，由于文本数据几乎总是产生大量的特征，所以在这种情况下逻辑回归并不是一个非常好的选择。

对于一个行外人来说，SVM的结果并不像决策树那样直观。同时使用非线性核，使得支持向量机在大型数据上的训练非常耗时。总之：

SVM的优点：

能够处理大型特征空间
能够处理非线性特征之间的相互作用
无需依赖整个数据
SVM的缺点：

当观测样本很多时，效率并不是很高
有时候很难找到一个合适的核函数
为此，我试着编写一个简单的工作流，决定应该何时选择这三种算法，流程如下：

首当其冲应该选择的就是逻辑回归，如果它的效果不怎么样，那么可以将它的结果作为基准来参考；
然后试试决策树（随机森林）是否可以大幅度提升模型性能。即使你并没有把它当做最终模型，你也可以使用随机森林来移除噪声变量；
如果特征的数量和观测样本特别多，那么当资源和时间充足时，使用SVM不失为一种选择。


最后，大家请记住，在任何时候好的数据总要胜过任何一个算法。时常思考下，看看是否可以使用你的领域知识来设计一个好的特征。在使用创建的特征做实验时，可以尝试下各种不同的想法。此外，你还可以尝试下多种模型的组合。这些我们将在下回讨论，所以，整装待发吧！
