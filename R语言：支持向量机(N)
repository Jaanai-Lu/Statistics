支持向量机是做什么的呢？
支持向量机（SVM）获取一个超平面将数据分成两类。以高水准要求来看，除了不会使用决策树以外，SVM与C4.5算法是执行相似的任务的。
超平面（hyperplane）是个函数，类似于解析一条线的方程。
实际上，对于只有两个属性的简单分类任务来说，超平面可以是一条线的。
其实事实证明：
SVM可以使用一个小技巧，把你的数据提升到更高的维度去处理。一旦提升到更高的维度中，SVM算法会计算出把你的数据分离成两类的最好的超平面。
有例子么？当然，举个最简单的例子。我发现桌子上开始就有一堆红球和蓝球，如果这这些球没有过分的混合在一起，不用移动这些球，你可以拿一根棍子把它们分离开。
你看，当在桌上加一个新球时，通过已经知道的棍子的哪一边是哪个颜色的球，你就可以预测这个新球的颜色了。
最酷的部分是什么呢？SVM算法可以算出这个超平面的方程。
如果事情变得更复杂该怎么办？当然了，事情通常都很复杂。如果球是混合在一起的，一根直棍就不能解决问题了。
下面是解决方案：
快速提起桌子，把所有的球抛向空中，当所有的球以正确的方式抛在空中时，你使用一张很大的纸在空中分开这些球。
你可能会想这是不是犯规了。不，提起桌子就等同于把你的数据映射到了高维空间中。
在这个例子中，我们从桌子表面的二维空间过渡到了球在空中的三维空间。
那么SVM该怎么做呢？
通过使用核函数（kernel），我们在高维空间也有很棒的操作方法。
这张大纸依然叫做超平面，但是现在它对应的方程是描述一个平面而不是一条线了。
根据Yuval的说法，一旦我们在三维空间处理问题，超平面肯定是一个面而不是线了。
那么在桌上或者空中的球怎么用现实的数据解释呢？
桌上的每个球都有自己的位置，我们可以用坐标来表示。
打个比方，一个球可能是距离桌子左边缘20cm 距离底部边缘50cm，另一种描述这个球的方式是使用坐标(x,y)或者(20,50)表达。x和y是代表球的两个维度。
可以这样理解：如果我们有个病人的数据集，每个病人可以用很多指标来描述，比如脉搏，胆固醇水平，血压等。每个指标都代表一个维度。
基本上，SVM把数据映射到一个更高维的空间然后找到一个能分类的超平面。
类间间隔(margin)经常会和SVM联系起来，类间间隔是什么呢？它是超平面和各自类中离超平面最近的数据点间的距离。在球和桌面的例子中，棍子和最近的红球和蓝球间的距离就是类间间隔(margin)。
SVM 的关键在于，它试图最大化这个类间间隔，使分类的超平面远离红球和蓝球。这样就能降低误分类的可能性。
那么支持向量机的名字是哪里来的？还是球和桌子的例子中，超平面到红球和蓝球的距离是相等的。这些球或者说数据点叫做支持向量，因为它们都是支持这个超平面的。
那这是监督算法还是非监督的呢？SVM 属于监督学习。因为开始需要使用一个数据集让 SVM学习这些数据中的类型。只有这样之后 SVM 才有能力对新数据进行分类。
为什么我们要用 SVM 呢？ SVM 和 C4.5大体上都是优先尝试的二类分类器。根据“没有免费午餐原理”，没有哪一种分类器在所有情况下都是最好的。此外，核函数的选择和可解释性是算法的弱点所在。
在哪里使用 SVM？有什么 SVM 的实现方法，比较流行的是用scikit-learn, MATLAB 和 libsvm实现的这几种。
