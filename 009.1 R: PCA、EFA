什么是主成分分析？
主成分的概念由Karl Pearson在1901年提出,是一种考察多个变量间相关性的多元统计方法，
其研究如何通过少数几个主成分(principal component)来解释多个变量间的内部结构。
即从原始变量中导出少数几个主分量，使它们尽可能多地保留原始变量的信息，且彼此间互不相关。
主成分分析的目的：数据的压缩；数据的解释。常被用来寻找判断事物或现象的综合指标，并对综合指标所包含的信息进行适当的解释。
主成分分析步骤：
对原来的p个指标进行标准化，以消除变量在水平和量纲上的影响
根据标准化后的数据矩阵求出相关系数矩阵
round(cor(data), 3) -> R
求出协方差矩阵的特征根和特征向量
确定主成分，并对各主成分所包含的信息给予适当的解释。
princomp(data, cor=T) -> PCA
summary(PCA, loading=T)
# 从输出的结果可以看出，主成分的标准差，即为相关矩阵的特征值的开方
# 通过主成分碎石图，可以直观地看出主成分的影响成分大小
# 预估成分影响
round(predict(PCA), 3)
# 画碎石图
screeplot(PCA, type='lines')
# 计算主成分得分
PCA$scores

R中的主成分和因子分析
R的基础安装包中提供了PCA和EFA的函数，分别为princomp（）和factanal（）
psych包中有用的因子分析函数
函数	描述　
principal（）	含多种可选的方差放置方法的主成分分析
fa（）	可用主轴、最小残差、加权最小平方或最大似然法估计的因子分析
fa.parallel（）	含平等分析的碎石图
factor.plot（）	绘制因子分析或主成分分析的结果
fa.diagram（）	绘制因子分析或主成分分析的载荷矩阵
scree（）	因子分析和主成分分析的碎石图

通过学习《R语言实战》关于这两种方法的解释，我们很容易理解这两种方法存在的意义——降维。
我们将要面对的数据实在是太大，变量实在太多，因此计算机所承受的压力也会越来越大。
信息过度复杂是多变量数据最大的挑战之一，特别是在还要考虑变量间交互关系的时候，变量增加时交互关系的量是按阶乘关系在往上涨的，
所以降维在很多时候能够起到减少大量工作量的作用，是数据分析很重要的一个思想。
以上是「主成分分析」与「因子分析」联系，有共同的目的之处。但是，两者的区别也很大，在实现目标时，两者采用了两种不同的思路，下面逐一讨论。
1. 主成分分析与因子分析的不同
主成分分析(PCA)是一种数据降维技巧，它能将大量相关变量转化为一组很少的不相关变量,同时尽可能保留初始变量的信息,这些无关变量称为主成分。
值得注意的是，主成分是初始变量的线性组合。
探索性因子分析（EFA）是一系列用来发现一组变量的潜在结构的方法，
通过发掘隐藏在数据下的一组较少的、更为基本的无法观测的变量，来解释一组可观测变量的相关性。
区别：从定义上其实就很好理解，但是更简单的理解方式是：
把「因子分析」看作是求得「元素」，这些元素是组成初始变量的因子；
而把「主成分分析」看作是求得一种「组合」，这些组合是初始变量配给不同系数组成的「组合」，且每个组合之间不相关。
2.主成分分析
2.1. 主成分
主成分是初始变量的线性组合，用于替代初始变量并尽可能保留初始信息。如第一主成分：
PC1 = a1X1 + a2X2 +...+ akXk
它是k个观测变量的加权组合，对初始变量集的方差解释性最大。
第二主成分是初始变量的线性组合，对方差的解释性排第二，同时与第一主成分正交（不相关）。
后面每一个主成分都最大化它对方差的解释程度，同时与之前所有的主成分都正交，但从实用的角度来看，都希望能用较少的主成分来近似全变量集。
至于主成分是如何推导得到的，请看这篇PCA（主成分分析）详解（写给初学者）(https://my.oschina.net/gujianhan/blog/225241#OSC_h2_1)
2.2. 判断主成分的个数
之前讲到，主成分分析的目的是为了降维，因此对于主成分的个数，我们必须有所取舍。
PCA中需要多少个主成分的准则：
根据先验经验和理论知识判断主成分数；
根据要解释变量方差的积累值的阈值来判断需要的主成分数；
通过检查变量间k*k的相关系数矩阵来判断保留的主成分数。
最常见的是基于特征值的方法，每个主成分都与相关系数矩阵的特征值关联，第一主成分与最大的特征值相关联，第二主成分与第二大的特征值相关联，依此类推。
判断主成分的方法通常有以下3种：
1. Kaiser-Harris准则，建议保留特征值大于1的主成分，特征值小于1的成分所解释的方差比包含在单个变量中的方差更少
2. Cattell碎石检验，通过绘制特征值与主成分数的图形，观察图形的弯曲情况，保留图形变化最大处之上的主成分
3. 通过模拟，依据与初始矩阵相同大小的随机数据矩阵来判断要提取的特征值，若基于真实数据的某个特征值大于一组随机数据矩阵相应的平均特征值，
则保留该主成分，该方法称作平行分析。
使用psych包中的fa.parallel( )函数可同时对三种特征值判别准则进行评价。
fa.parallel(USJudgeRatings[,-1], fa='pc', n.iter=100, show.legend=FALSE, main="Scree plot with parallel analysis")
碎石头、特征值大于1准则和100次模拟的平行分析（虚线）都表明保留一个主成分即可保留数据集的大部分信息，下一步是使用principal（）函数挑选出相应的主成分。
2.3. 提取主成分
通过R中已经包装好的算法来计算即可：
principal(r，nfactors=, rotate=, scores)
# r 是相关系数矩阵或原始数据矩阵
# nfactors 设定主成分数,默认为1
# rotate 指定旋转的方法,默认最大方差旋转（varimax）
# scores 设定是否计算主成分得分（默认为不需要）
principal(r = USJudgeRatings[, -1], nfactors = 1)
通过以上函数，我们可以得如下的结果
Principal Components Analysis
Call: principal(r = USJudgeRatings[, -1], nfactors = 1)
Standardized loadings (pattern matrix) based upon correlation matrix
     PC1   h2     u2 com
INTG 0.92 0.84 0.1565   1
DMNR 0.91 0.83 0.1663   1
DILG 0.97 0.94 0.0613   1
CFMG 0.96 0.93 0.0720   1
DECI 0.96 0.92 0.0763   1
PREP 0.98 0.97 0.0299   1
FAMI 0.98 0.95 0.0469   1
ORAL 1.00 0.99 0.0091   1
WRIT 0.99 0.98 0.0196   1
PHYS 0.89 0.80 0.2013   1
RTEN 0.99 0.97 0.0275   1
                PC1
SS loadings    10.13
Proportion Var  0.92
Mean item complexity =  1
Test of the hypothesis that 1 component is sufficient.
The root mean square of the residuals (RMSR) is  0.04 
 with the empirical chi square  6.21  with prob <  1 
Fit based upon off diagonal values = 1
由于PCA只对相关系数矩阵进行分析，在获取主成分前，原始数据将会被自动转换为相关系数矩阵。
PC1 栏包含了成分载荷，即观测变量与主成分的相关系数，如果主成分不止一个，还会有PC2、PC3。
成分载荷（component loadings）可用来解释主成分的含义。
此处可看到，第一主成分（PC1）与每个变量都高度相关，也就是说，它是一个可用来进行一般性评价的维度。
h2 栏指成分公因子方差，即主成分对每个变量的方差解释度。
u2 栏指成分的唯一性，即方差无法被主成分解释的比例（1 - h2） 。
SS loadings行包含了主成分相关联的特征值，指的是与特定主成分相关联的标准化后的方差值。
Proportion Var行表示的是每个主成分对整个数据集的解释程度。
结果不止一个主成分的情况:
fa.parallel(Harman23.cor$cov, n.obs=302, fa="pc", n.iter=100, show.legend=FALSE, main="Scree plot with parallel analysis")
当提取了多个成分时，对它们进行旋转可使结果更具有解释性。
旋转是一系列将成分载荷阵变得更容易解释的数学方法，它们尽可能地对成分去噪。
旋转方法有两种：使选择的成分保持不相关（正效旋转），和让它们变得相关（斜交旋转）。
旋转方法也会依据去噪定义的不同而不同。
最流行的正交旋转是方差极大旋转，它试图对载荷阵的列进行去噪，
使得每个成分只是由一组有限的变量来解释（即载荷阵每列只有少数几个很大的载荷，其他都是很小的载荷）。
使用方差极大旋转
principal(r = Harman23.cor$cov, nfactors = 2, rotate = "varimax")
可以得到如下结果：
Principal Components Analysis
Call: principal(r = Harman23.cor$cov, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
               RC1  RC2   h2    u2 com
height         0.90 0.25 0.88 0.123 1.2
arm.span       0.93 0.19 0.90 0.097 1.1
forearm        0.92 0.16 0.87 0.128 1.1
lower.leg      0.90 0.22 0.86 0.139 1.1
weight         0.26 0.88 0.85 0.150 1.2
bitro.diameter 0.19 0.84 0.74 0.261 1.1
chest.girth    0.11 0.84 0.72 0.283 1.0
chest.width    0.26 0.75 0.62 0.375 1.2
                      RC1  RC2
SS loadings           3.52 2.92
Proportion Var        0.44 0.37
Cumulative Var        0.44 0.81
Proportion Explained  0.55 0.45
Cumulative Proportion 0.55 1.00
Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.
The root mean square of the residuals (RMSR) is  0.05 
Fit based upon off diagonal values = 0.99
列名由 PC 变成 RC，表示成分被旋转。
通过观察可以看出，主成分对初始变量的关联程度得到了分化，第一主成分主要由前四个变量来解释（长度变量），第二主成分主要由后四个变量来解释（容量变量）。
注意两个主成分仍不相关，对变量的解释性不变，这是因为变量的群组没有发生变化。
另外，两个主成分放置后的累积方差解释性没有变化，变的只是各个主成分对方差的解释（成分1从58%变为44%，成分2从22%变为37%）。
各成分的方差解释度趋同，准确来说，此时应该称它们为成分而不是主成分。
主成分得分，是用于评价不同观测之间优劣的分数。
1. 如果用于主成分分析的初始数据矩阵我们知道（可能不知道，直接用相关系数矩阵来进行主成分分析），则可以直接添加score参数来计算每个观测的主成分得分了。
当scores=TRUE时，主成分得分存储在principal（）函数返回对象的scores元素中。
利用principal（）函数，很容易获得每个调查对象在该主成分上的得分。
从原始数据中获取成分得分
principal(USJudgeRatings[,-1], nfactors=1, score=TRUE) -> pc
head(pc$scores)
2. 如果初始数据矩阵未知，我们就没办法直接通过参数调节来得到原来观测的主成分得分了，要通过计算得分系数之后，得到主成分得分的求得公式。例：
获取主成分得分的系数
principal(Harman23.cor$cov , nfactors = 2, rotate = "varimax") -> rc
round(unclass(rc$weights), 2)
                RC1   RC2
height          0.28 -0.05
arm.span        0.30 -0.08
forearm         0.30 -0.09
lower.leg       0.28 -0.06
weight         -0.06  0.33
bitro.diameter -0.08  0.32
chest.girth    -0.10  0.34
chest.width    -0.04  0.27
得到的主成分得分：
PC1 = 0.28*height + 0.30*arm.span + 0.30*forearm + 0.28*lower.leg - 0.06*weight - 0.08*bitro.diameter - 0.10*chest.girth - 0.04*chest.width
PC2 = -0.05*height - 0.08*arm.span - 0.09*forearm - 0.06*lower.leg + 0.33*weight + 0.32*bitro.diameter + 0.34*chest.girth + 0.27*chest.width
2.4. 评价
主成分得分就可看着是各观测的主成分取值，它是初始变量的线性组合。通过得分的排名来得到观测的优劣排名。
3. 因子分析
3.1. 因子
EFA的目标是通过发掘隐藏在数据下的一组较少的、更为基本的无法观测的变量，来解释一组可观测变量的相关性。这些虚拟的、无法观测的变量称作因子。
（每个因子被认为可解释多个观测变量间共有的方差，也叫作公共因子）
直观意义上讲，可以将观测变量视为是因子的线性组合（注意：主成分是观测变量的线性组合，容易弄混淆）。
如第i个观测变量可表示为：
Xi = a1F1 + a2F2 +...+ apFp + Ui
这里的Xi是第i个可观测变量（i=1,2,……k）
Fj(j=1,2,....p) 就是我们说的因子，Ui则指的是第i个观测变量独有的部分（无法被因子解释），并且p<k
3.2. 判断需提取的公共因子数
这里的方法和判断主成分个数相同，同样采用fa.parallel ( )函数来得到图形，使用三种方法来综合判断。
唯一一点不同在于，采用Kaiser-Harris准则来判断时，主成分分析保留特征值大于1的成分，而因子分析保留特征值大于0的因子。
options(digits = 2)
# ability.cov提供了变量的协方差矩阵
ability.cov$cov -> covariances
# cov2cor（）函数将其转化为相关系数矩阵
cov2cor(covariances) -> correlations
library(pacman)
p_load(psych)
fa.parallel(correlations, n.obs=112, fa="both", n.iter=100, main="Scree plots with parallel analysis") 
若使用PCA方法，可能会选择一个成分或两个成分。当摇摆不定时，高估因子数通常比低估因子数的结果好，因为高估因子数一般较少曲解“真实”情况。
3.3. 提取公共因子
这里采用R语言中包装好的算法来计算即可：
fa（r, nfactors=, n.obs=, rotate=, scores=, fm=)
# 相比主成分分析的 principle 函数，这里多出了两个参数
# n.obs 指的是观测数，在输入相关系数矩阵时需要填写
# fm 则用于设定因子化方法（默认极小残差法）
r是相关系数矩阵或原始数据矩阵；
nfactors设定提取的因子数（默认为1）；
rotate设定放置的方法（默认互变异数最小法）；
scores设定是否计算因子得分（默认不计算）；
与PCA不同，提取公共因子的方法很多，包括：
最大似然法（ ml：Maximum Likelihood ）
主轴迭代法（ pa：Principal Axis ）
加权最小二乘法（ wls：Weighted Least Squares ）
广义加权最小二乘法（ gls ）
最小残差法（ minres：minimum residual ）
fa(correlations, nfactors=2, rotate="none", fm="pa") -> fa
fa
3.4. 因子旋转
因子旋转的选择有三种：1. 不旋转 2. 正交旋转 3. 斜交旋转
不旋转时得到各因子与各初始变量之间的相关程度，旋转之后能够使因子变得更好解释。
正交旋转
# 用正交旋转提取因子
fa(correlations, nfactors=2, rotate="varimax", fm="pa") -> fa.varimax
fa.varimax
令参数 rotate="varimax" 即可，这时的因子之间强制为不相关
在提取因子后，可得到因子结构矩阵，即变量与因子的相关系数，较不旋转的结果而言更好解释，可将不同变量重点归于某一因子。
正交旋转，因子分析的重点在于因子结构矩阵（变量与因子的相关系数） 
斜交旋转
# 用斜交旋转提取因子
p_load(GPArotation)
fa(correlations, nfactors=2, rotate="promax", fm="pa") -> fa.promax
fa.promax
令参数 rotate="promax" 即可，这时的因子之间有可能是相关的。
在提取因子后，可得到两个矩阵：因子模式矩阵（标准化的回归系数矩阵，它列出了因子的预测变量的权重）、因子关联矩阵（因子相关系数矩阵）。
同时还需要考虑因子结构矩阵（变量与因子的相关系数矩阵），可使用公式F=P*Phi来计算得到，其中F是载荷阵，P为因子模式矩阵，Phi为因子关联矩阵。
通过自编函数 fsm ( )可以用于该乘法计算,自编函数的具体代码参考《R语言实战》一书。可以看到变量与因子间的相关系数。
将它们与正交旋转所得因子载荷阵相比，发现该载荷阵列的噪音较大，这是因为之前允许潜在因子相关。虽然斜交方法更为复杂，但模型将更加符合真实数据。
通过考虑以上三个矩阵，我们除了可以将不同变量重点归于某一因子外，还可以得到不同因子之间的相关系数。
通过图形显示
可通过factor.plot ( )函数来得到两因子图形，可从图中观测出不同初始变量在哪个因子上载荷更大。
factor.plot(fa.promax, labels=rownames(fa.promax$loadings))
还可通过fa.diagram ( )函数得到每个因子下的最大载荷，以及因子间的相关系数。
fa.diagram(fa.promax, simple=TRUE)  
3.5. 因子得分
EFA并不十分关注因子得分,想要得到因子得分，在提取时采用score=TRUE函数即可，还可以得到得分系数（标准化的回归权重），
在提取因子时得分系数将会体现在返回对象weights元素中。
fa.promax$weights
而且，这里与主成分分析不同，因子得分无法精确计算，只能估计得到。
原因在于，我们的因子数总比初始变量要少，通常因子得分的估计方法有：回归估计法，Bartlett估计法，Thomson估计法。
3.6. 评价
以各因子的方差贡献率为权，由各因子的线性组合得到综合评价指标函数。
F = (w1F1+w2F2+…+wmFm)/(w1+w2+…+wm )
此处wi为旋转前或旋转后因子的方差贡献率。
利用综合得分可以得到得分名次。
4. 其他
对因子分析非常有用的软件包，FactoMineR包不仅提供了PCA和EFA方法，还包含潜变量模型。
FAiR包使用遗传算法来估计因子分析模型，增强了模型参数估计能力，能够处理不等式的约束条件；
GPArotation包提供了许多因子旋转方法，
nFactors包，提供了用来判断因子数目方法。
先验知识的模型：先从一些先验知识开始，比如变量背后有几个因子、变量在因子上的载荷是怎样的、因子间的相关性如何，
然后通过收集数据检验这些先验知识。这种方法称作验证性因子分析（CFA）。做CFA的软件包：sem、openMx和lavaan等。
ltm包可以用来拟合测验和问卷中各项目的潜变量模型。
潜类别模型（潜在的因子被认为是类别型而非连续型）可通过FlexMix、lcmm、randomLCA和poLC包进行拟合。
lcda包可做潜类别判别分析，而lsa可做潜在语义分析----一种自然语言处理中的方法。ca包提供了可做简单和多重对应分析的函数。
R中还包含了众多的多维标度法（MDS）计算工具。MDS即可用发现解释相似性和可测对象间距离的潜在维度。
cmdscale（）函数可做经典的MDS
MASS包中的isoMDS（）函数可做非线性MDS
vagan包中则包含了两种MDS的函数
5. 总结
主成分分析和因子分析的步骤可归纳为：
1. 数据预处理:
PCA和EFA都是根据观测变量间的相关性来推导结果。
我们可以输入原始数据矩阵或相关系数矩阵列到principal()和fa（）函数中，若输入初始结果，相关系数矩阵将会被自动计算，在计算前请确保数据中没有缺失值；
2. 选择模型，是主成分分析还是因子分析：
判断是PCA（数据降维）还是EFA（发现潜在结构）更符合你的分析目标。若选择EFA方法时，还需要选择一种估计因子模型的方法（如最大似然估计）。
3. 判断要选择的主成分/因子数目
4. 选择主成分/因子
5. 旋转主成分/因子
6. 解释结果
7. 计算主成分/因子得分

本文的目的是为主成分分析（PCA）提供一个完整且简单的解释，特别是其运作方式，以增进大家对该分析法的理解并加以利用，而不必具有强大的数学背景。
PCA实际上是网上广泛提及的一种方法，很多文章都有涉及。
在开始解释之前，本文提供了PCA在每一步骤的运作原理的逻辑解释，简化了其背后的数学概念，如标准化，协方差，特征向量和特征值，而暂未关注如何运算的问题。
PCA是一种常用于减少大数据集维数的降维方法，把大变量集转换为仍包含大变量集中大部分信息的较小变量集。
减少数据集的变量数量，自然是以牺牲精度为代价的，降维的好处是以略低的精度换取简便。
因为较小的数据集更易于探索和可视化，并且使机器学习算法更容易和更快地分析数据，而不需处理无关变量。
总而言之，PCA的概念很简单——减少数据集的变量数量，同时保留尽可能多的信息。
逐步解释
第1步：标准化
这一步的目的是把输入数据集变量的范围标准化，以使它们中的每一个均可大致成比例地分析。
更具体地说，在使用PCA之前必须标准化数据的原因是PCA对初始变量的方差非常敏感。
也就是说，如果初始变量的范围之间存在较大差异，那么范围较大的变量将占据范围较小的变量（例如，范围介于0和100之间的变量将占据0到1之间的变量），
这将导致主成分的偏差。因此，将数据转换为可比较的比例可避免此问题。
在数学上，这一步可以通过减去平均值，再除以每个变量值的标准偏差来完成。
z = (value - mean)/standard deviation
只要标准化完成后，所有变量都将转换为相同的范围[0,1]。
第2步：协方差矩阵计算
这一步的目的是：了解输入数据集的变量是如何相对于平均值变化的。
或者换句话说，是为了查看它们之间是否存在任何关系。
因为有时候，变量间高度相关是因为它们包含大量的信息。
因此，为了识别这些相关性，我们进行协方差矩阵计算。
协方差矩阵是p×p对称矩阵（其中p是维数），其所有可能的初始变量与相关联的协方差作为条目。
例如，对于具有3个变量x，y和z的三维数据集，协方差矩阵是3×3矩阵。
由于变量与其自身的协方差是其方差（Cov（a，a）= Var（a）），
因此在主对角线（左上角到右下角）中，实际上有每个起始变量的方差。
并且由于协方差是可交换的（Cov（a，b）= Cov（b，a）），所以协方差矩阵的条目相对于主对角线是对称的，这意味着上三角形部分和下三角形部分是相等的。
作为矩阵条目的协方差告诉我们变量之间的相关性是什么呢？
协方差的重要标志如下：
如果为正，则两个变量同时增加或减少
如果为负，则一个减少，另一个增加
好了，现在我们知道协方差矩阵只不过是一个表，汇总了所有可能配对的变量间相关性。让我们继续下一步。
第3步：计算协方差矩阵的特征向量和特征值，用以识别主成分
特征向量和特征值都是线性代数概念，需要从协方差矩阵计算得出，以便确定数据的主成分。
开始解释这些概念之前，让我们首先理解主成分的含义。
主成分是由初始变量的线性组合或混合构成的新变量。
该组合中新变量（如主成分）之间彼此不相关，且大部分初始变量都被压缩进首个成分中。
所以，10维数据会显示10个主成分，但是PCA试图在第一个成分中得到尽可能多的信息，然后在第二个成分中得到尽可能多的剩余信息，以此类推。
例如，假设你有一个10维数据，你最终将得到的主成分中第一个主成分包含原始数据集的大部分信息，而最后一个主成分只包含其中的很少部分。
因此，以这种方式组织信息，可以在不丢失太多信息的情况下减少维度，而这需要丢弃携带较少信息的成分。
要认识到一件重要的事情是，既然新变量被构造为初始变量的线性组合，它们将更加难以解释，并且对我们没有任何实际意义。
从几何学上讲，主成分代表了解释最大方差量的数据方向，也就是说，它们是捕获数据中大部分信息的线。
在这里，方差和信息间的关系是，线所承载的方差越大，数据点沿着它的分散也越大，沿着线的散点越多，它所携带的信息也越多。
简单地说，只要把主成分看作是提供最佳角度来观察和评估数据的新轴，这样观测结果之间的差异就会更明显。
PCA如何构建主成分？
由于主成分的数量，如同数据中存在的变量一样多，因此主成分根据第一主成分占数据集中最大可能方差的方式进行构造。
例如，假设我们的数据集的散点图如下所示，可以猜出第一个主成分吗？
是的，就是大致与紫色标记匹配的线。
因为它穿过原点，并且它是点（红点）的投影最分散的线。或者从数学上来讲，它是方差最大化的线（从投影点（红点）到原点的平方距离的平均值）。
第二主成分以相同的方式计算，条件是它与第一主成分并不相关（即垂直），并且它占第二高方差。
直到计算出p个主成分数量，等于原始变量数。
现在我们理解了主成分的含义，让我们回到特征向量和特征值。
首先，你需要知道的是它们总是成对出现，因此每个特征向量都有一个特征值，它们的数量等于数据的维数。
例如，对于三维数据集，存在3个变量，因此存在3个具有对应特征值的特征向量。
不用多说，上面解释的所有“魔法”都是特征向量和特征值，因为协方差矩阵的特征向量实际上是方差最多的轴的方向（或最多的信息），我们称之为主成分。
并且，特征值只是附加到特征向量上的系数，它们给出了每个主成分中携带的方差量。
通过特征值的顺序对特征向量进行排序，从最高到最低，你就得到了按重要性排序的主成分。
举例：
假设我们的数据集是2维的，有2个变量x，y，并且协方差矩阵的特征向量和特征值如下：
如果我们按降序对特征值进行排序，则得到λ1>λ2，这意味着与第一主成分（PC1）对应的特征向量是v1，而与第二成分（PC2）对应的特征向量是v2。
在有了主成分之后，为了计算每个成分所占的方差（信息）百分比，我们将每个成分的特征值除以特征值的总和。
如果我们把这个计算方法应用到上面的例子中，我们会发现，PC1和PC2分别携带了96％和4％的数据方差。
第4步：特征向量
正如我们在上一步中所看到的，计算特征向量并按其特征值依降序排列，使我们能够按重要性顺序找到主成分。
在这个步骤中我们要做的，是选择保留所有成分还是丢弃那些重要性较低的成分（低特征值），并与其他成分形成一个向量矩阵，我们称之为特征向量。
因此，特征向量只是一个矩阵，其中包含我们决定保留的成分的特征向量作为列。
这是降维的第一步，因为如果我们选择只保留n个特征向量（分量）中的p个，则最终数据集将只有p维。
举例：
接着上一步的例子，我们可以用v1或v2向量来形成一个特征向量。
或者丢弃重要性较小的向量v2,仅用v1形成一个特征向量。
丢弃特征向量v2将使维数减少1，并且将导致最终数据集中的信息丢失。   
但鉴于v2仅携带4％的信息，因此损失并不重要，我们仍将拥有v1所携带的96％的信息。
因此，正如我们在例子中看到的那样，你可以选择是保留所有成分还是丢弃不重要的成分，具体取决于你要查找的内容。
如果你不追求降维，只是想利用不相关的新变量（主成分）描述你的数据，则不需要保留重要性较次的成分。
最后一步：沿主成分轴重新绘制数据
在前面的步骤中，除了标准化之外，你不需要更改任何数据，只需选择主成分，形成特征向量，但输入数据集时要始终与原始轴统一（即初始变量）。
这一步，也是最后一步，目标是使用协方差矩阵的特征向量去形成新特征向量，将数据从原始轴重新定位到由主成分轴中（因此称为主成分分析）。
这可以通过将原始数据集的转置乘以特征向量的转置来完成。
