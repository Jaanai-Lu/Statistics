什么是主成分分析？
主成分的概念由Karl Pearson在1901年提出,是一种考察多个变量间相关性的多元统计方法，
其研究如何通过少数几个主成分(principal component)来解释多个变量间的内部结构。
即从原始变量中导出少数几个主分量，使它们尽可能多地保留原始变量的信息，且彼此间互不相关。
主成分分析的目的：数据的压缩；数据的解释。常被用来寻找判断事物或现象的综合指标，并对综合指标所包含的信息进行适当的解释。
主成分分析步骤：
对原来的p个指标进行标准化，以消除变量在水平和量纲上的影响
根据标准化后的数据矩阵求出相关系数矩阵
round(cor(data), 3) -> R
求出协方差矩阵的特征根和特征向量
确定主成分，并对各主成分所包含的信息给予适当的解释。
princomp(data, cor=T) -> PCA
summary(PCA, loading=T)
# 从输出的结果可以看出，主成分的标准差，即为相关矩阵的特征值的开方
# 通过主成分碎石图，可以直观地看出主成分的影响成分大小
# 预估成分影响
round(predict(PCA), 3)
# 画碎石图
screeplot(PCA, type='lines')
# 计算主成分得分
PCA$scores

R中的主成分和因子分析
R的基础安装包中提供了PCA和EFA的函数，分别为princomp（）和factanal（）
psych包中有用的因子分析函数
函数	描述　
principal（）	含多种可选的方差放置方法的主成分分析
fa（）	可用主轴、最小残差、加权最小平方或最大似然法估计的因子分析
fa.parallel（）	含平等分析的碎石图
factor.plot（）	绘制因子分析或主成分分析的结果
fa.diagram（）	绘制因子分析或主成分分析的载荷矩阵
scree（）	因子分析和主成分分析的碎石图

通过学习《R语言实战》关于这两种方法的解释，我们很容易理解这两种方法存在的意义——降维。
我们将要面对的数据实在是太大，变量实在太多，因此计算机所承受的压力也会越来越大。
信息过度复杂是多变量数据最大的挑战之一，特别是在还要考虑变量间交互关系的时候，变量增加时交互关系的量是按阶乘关系在往上涨的，
所以降维在很多时候能够起到减少大量工作量的作用，是数据分析很重要的一个思想。
以上是「主成分分析」与「因子分析」联系，有共同的目的之处。但是，两者的区别也很大，在实现目标时，两者采用了两种不同的思路，下面逐一讨论。
1. 主成分分析与因子分析的不同
主成分分析(PCA)是一种数据降维技巧，它能将大量相关变量转化为一组很少的不相关变量,同时尽可能保留初始变量的信息,这些无关变量称为主成分。
值得注意的是，主成分是初始变量的线性组合。
探索性因子分析（EFA）是一系列用来发现一组变量的潜在结构的方法，
通过发掘隐藏在数据下的一组较少的、更为基本的无法观测的变量，来解释一组可观测变量的相关性。
区别：从定义上其实就很好理解，但是更简单的理解方式是：
把「因子分析」看作是求得「元素」，这些元素是组成初始变量的因子；
而把「主成分分析」看作是求得一种「组合」，这些组合是初始变量配给不同系数组成的「组合」，且每个组合之间不相关。
2.主成分分析
2.1. 主成分
主成分是初始变量的线性组合，用于替代初始变量并尽可能保留初始信息。如第一主成分：
PC1 = a1X1 + a2X2 +...+ akXk
它是k个观测变量的加权组合，对初始变量集的方差解释性最大。
第二主成分是初始变量的线性组合，对方差的解释性排第二，同时与第一主成分正交（不相关）。
后面每一个主成分都最大化它对方差的解释程度，同时与之前所有的主成分都正交，但从实用的角度来看，都希望能用较少的主成分来近似全变量集。
至于主成分是如何推导得到的，请看这篇PCA（主成分分析）详解（写给初学者）(https://my.oschina.net/gujianhan/blog/225241#OSC_h2_1)
2.2. 判断主成分的个数
之前讲到，主成分分析的目的是为了降维，因此对于主成分的个数，我们必须有所取舍。
PCA中需要多少个主成分的准则：
根据先验经验和理论知识判断主成分数；
根据要解释变量方差的积累值的阈值来判断需要的主成分数；
通过检查变量间k*k的相关系数矩阵来判断保留的主成分数。
最常见的是基于特征值的方法，每个主成分都与相关系数矩阵的特征值关联，第一主成分与最大的特征值相关联，第二主成分与第二大的特征值相关联，依此类推。
判断主成分的方法通常有以下3种：
1. Kaiser-Harris准则，建议保留特征值大于1的主成分，特征值小于1的成分所解释的方差比包含在单个变量中的方差更少
2. Cattell碎石检验，通过绘制特征值与主成分数的图形，观察图形的弯曲情况，保留图形变化最大处之上的主成分
3. 通过模拟，依据与初始矩阵相同大小的随机数据矩阵来判断要提取的特征值，若基于真实数据的某个特征值大于一组随机数据矩阵相应的平均特征值，
则保留该主成分，该方法称作平行分析。
使用psych包中的fa.parallel( )函数可同时对三种特征值判别准则进行评价。
fa.parallel(USJudgeRatings[,-1], fa='pc', n.iter=100, show.legend=FALSE, main="Scree plot with parallel analysis")
碎石头、特征值大于1准则和100次模拟的平行分析（虚线）都表明保留一个主成分即可保留数据集的大部分信息，下一步是使用principal（）函数挑选出相应的主成分。
2.3. 提取主成分
通过R中已经包装好的算法来计算即可：
principal(r，nfactors=, rotate=, scores)
# r 是相关系数矩阵或原始数据矩阵
# nfactors 设定主成分数,默认为1
# rotate 指定旋转的方法,默认最大方差旋转（varimax）
# scores 设定是否计算主成分得分（默认为不需要）
principal(r = USJudgeRatings[, -1], nfactors = 1)
通过以上函数，我们可以得如下的结果
Principal Components Analysis
Call: principal(r = USJudgeRatings[, -1], nfactors = 1)
Standardized loadings (pattern matrix) based upon correlation matrix
     PC1   h2     u2 com
INTG 0.92 0.84 0.1565   1
DMNR 0.91 0.83 0.1663   1
DILG 0.97 0.94 0.0613   1
CFMG 0.96 0.93 0.0720   1
DECI 0.96 0.92 0.0763   1
PREP 0.98 0.97 0.0299   1
FAMI 0.98 0.95 0.0469   1
ORAL 1.00 0.99 0.0091   1
WRIT 0.99 0.98 0.0196   1
PHYS 0.89 0.80 0.2013   1
RTEN 0.99 0.97 0.0275   1
                PC1
SS loadings    10.13
Proportion Var  0.92
Mean item complexity =  1
Test of the hypothesis that 1 component is sufficient.
The root mean square of the residuals (RMSR) is  0.04 
 with the empirical chi square  6.21  with prob <  1 
Fit based upon off diagonal values = 1
由于PCA只对相关系数矩阵进行分析，在获取主成分前，原始数据将会被自动转换为相关系数矩阵。
PC1 栏包含了成分载荷，即观测变量与主成分的相关系数，如果主成分不止一个，还会有PC2、PC3。
成分载荷（component loadings）可用来解释主成分的含义。
此处可看到，第一主成分（PC1）与每个变量都高度相关，也就是说，它是一个可用来进行一般性评价的维度。
h2 栏指成分公因子方差，即主成分对每个变量的方差解释度。
u2 栏指成分的唯一性，即方差无法被主成分解释的比例（1 - h2） 。
SS loadings行包含了主成分相关联的特征值，指的是与特定主成分相关联的标准化后的方差值。
Proportion Var行表示的是每个主成分对整个数据集的解释程度。
结果不止一个主成分的情况:
fa.parallel(Harman23.cor$cov, n.obs=302, fa="pc", n.iter=100, show.legend=FALSE, main="Scree plot with parallel analysis")
当提取了多个成分时，对它们进行旋转可使结果更具有解释性。
旋转是一系列将成分载荷阵变得更容易解释的数学方法，它们尽可能地对成分去噪。
旋转方法有两种：使选择的成分保持不相关（正效旋转），和让它们变得相关（斜交旋转）。
旋转方法也会依据去噪定义的不同而不同。
最流行的正交旋转是方差极大旋转，它试图对载荷阵的列进行去噪，
使得每个成分只是由一组有限的变量来解释（即载荷阵每列只有少数几个很大的载荷，其他都是很小的载荷）。
使用方差极大旋转
principal(r = Harman23.cor$cov, nfactors = 2, rotate = "varimax")
可以得到如下结果：
Principal Components Analysis
Call: principal(r = Harman23.cor$cov, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
               RC1  RC2   h2    u2 com
height         0.90 0.25 0.88 0.123 1.2
arm.span       0.93 0.19 0.90 0.097 1.1
forearm        0.92 0.16 0.87 0.128 1.1
lower.leg      0.90 0.22 0.86 0.139 1.1
weight         0.26 0.88 0.85 0.150 1.2
bitro.diameter 0.19 0.84 0.74 0.261 1.1
chest.girth    0.11 0.84 0.72 0.283 1.0
chest.width    0.26 0.75 0.62 0.375 1.2
                      RC1  RC2
SS loadings           3.52 2.92
Proportion Var        0.44 0.37
Cumulative Var        0.44 0.81
Proportion Explained  0.55 0.45
Cumulative Proportion 0.55 1.00
Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.
The root mean square of the residuals (RMSR) is  0.05 
Fit based upon off diagonal values = 0.99
列名由 PC 变成 RC，表示成分被旋转。
通过观察可以看出，主成分对初始变量的关联程度得到了分化，第一主成分主要由前四个变量来解释（长度变量），第二主成分主要由后四个变量来解释（容量变量）。
注意两个主成分仍不相关，对变量的解释性不变，这是因为变量的群组没有发生变化。
另外，两个主成分放置后的累积方差解释性没有变化，变的只是各个主成分对方差的解释（成分1从58%变为44%，成分2从22%变为37%）。
各成分的方差解释度趋同，准确来说，此时应该称它们为成分而不是主成分。
主成分得分，是用于评价不同观测之间优劣的分数。
1. 如果用于主成分分析的初始数据矩阵我们知道（可能不知道，直接用相关系数矩阵来进行主成分分析），则可以直接添加score参数来计算每个观测的主成分得分了。
当scores=TRUE时，主成分得分存储在principal（）函数返回对象的scores元素中。
利用principal（）函数，很容易获得每个调查对象在该主成分上的得分。
从原始数据中获取成分得分
principal(USJudgeRatings[,-1], nfactors=1, score=TRUE) -> pc
head(pc$scores)
2. 如果初始数据矩阵未知，我们就没办法直接通过参数调节来得到原来观测的主成分得分了，要通过计算得分系数之后，得到主成分得分的求得公式。例：
获取主成分得分的系数
principal(Harman23.cor$cov , nfactors = 2, rotate = "varimax") -> rc
round(unclass(rc$weights), 2)
                RC1   RC2
height          0.28 -0.05
arm.span        0.30 -0.08
forearm         0.30 -0.09
lower.leg       0.28 -0.06
weight         -0.06  0.33
bitro.diameter -0.08  0.32
chest.girth    -0.10  0.34
chest.width    -0.04  0.27
得到的主成分得分：
PC1 = 0.28*height + 0.30*arm.span + 0.30*forearm + 0.28*lower.leg - 0.06*weight - 0.08*bitro.diameter - 0.10*chest.girth - 0.04*chest.width
PC2 = -0.05*height - 0.08*arm.span - 0.09*forearm - 0.06*lower.leg + 0.33*weight + 0.32*bitro.diameter + 0.34*chest.girth + 0.27*chest.width
2.4. 评价
主成分得分就可看着是各观测的主成分取值，它是初始变量的线性组合。通过得分的排名来得到观测的优劣排名。
3. 因子分析
3.1. 因子
EFA的目标是通过发掘隐藏在数据下的一组较少的、更为基本的无法观测的变量，来解释一组可观测变量的相关性。这些虚拟的、无法观测的变量称作因子。
（每个因子被认为可解释多个观测变量间共有的方差，也叫作公共因子）
直观意义上讲，可以将观测变量视为是因子的线性组合（注意：主成分是观测变量的线性组合，容易弄混淆）。
如第i个观测变量可表示为：
Xi = a1F1 + a2F2 +...+ apFp + Ui
这里的Xi是第i个可观测变量（i=1,2,……k）
Fj(j=1,2,....p) 就是我们说的因子，Ui则指的是第i个观测变量独有的部分（无法被因子解释），并且p<k
3.2. 判断需提取的公共因子数
这里的方法和判断主成分个数相同，同样采用fa.parallel ( )函数来得到图形，使用三种方法来综合判断。
唯一一点不同在于，采用Kaiser-Harris准则来判断时，主成分分析保留特征值大于1的成分，而因子分析保留特征值大于0的因子。
options(digits = 2)
# ability.cov提供了变量的协方差矩阵
ability.cov$cov -> covariances
# cov2cor（）函数将其转化为相关系数矩阵
cov2cor(covariances) -> correlations
library(pacman)
p_load(psych)
fa.parallel(correlations, n.obs=112, fa="both", n.iter=100, main="Scree plots with parallel analysis") 
若使用PCA方法，可能会选择一个成分或两个成分。当摇摆不定时，高估因子数通常比低估因子数的结果好，因为高估因子数一般较少曲解“真实”情况。
3.3. 提取公共因子
这里采用R语言中包装好的算法来计算即可：
fa（r, nfactors=, n.obs=, rotate=, scores=, fm=)
# 相比主成分分析的 principle 函数，这里多出了两个参数
# n.obs 指的是观测数，在输入相关系数矩阵时需要填写
# fm 则用于设定因子化方法（默认极小残差法）
r是相关系数矩阵或原始数据矩阵；
nfactors设定提取的因子数（默认为1）；
rotate设定放置的方法（默认互变异数最小法）；
scores设定是否计算因子得分（默认不计算）；
与PCA不同，提取公共因子的方法很多，包括：
最大似然法（ ml：Maximum Likelihood ）
主轴迭代法（ pa：Principal Axis ）
加权最小二乘法（ wls：Weighted Least Squares ）
广义加权最小二乘法（ gls ）
最小残差法（ minres：minimum residual ）
fa(correlations, nfactors=2, rotate="none", fm="pa") -> fa
fa
3.4. 因子旋转
因子旋转的选择有三种：1. 不旋转 2. 正交旋转 3. 斜交旋转
不旋转时得到各因子与各初始变量之间的相关程度，旋转之后能够使因子变得更好解释。
正交旋转
# 用正交旋转提取因子
fa(correlations, nfactors=2, rotate="varimax", fm="pa") -> fa.varimax
fa.varimax
令参数 rotate="varimax" 即可，这时的因子之间强制为不相关
在提取因子后，可得到因子结构矩阵，即变量与因子的相关系数，较不旋转的结果而言更好解释，可将不同变量重点归于某一因子。
正交旋转，因子分析的重点在于因子结构矩阵（变量与因子的相关系数） 
斜交旋转
# 用斜交旋转提取因子
p_load(GPArotation)
fa(correlations, nfactors=2, rotate="promax", fm="pa") -> fa.promax
fa.promax
令参数 rotate="promax" 即可，这时的因子之间有可能是相关的。
在提取因子后，可得到两个矩阵：因子模式矩阵（标准化的回归系数矩阵，它列出了因子的预测变量的权重）、因子关联矩阵（因子相关系数矩阵）。
同时还需要考虑因子结构矩阵（变量与因子的相关系数矩阵），可使用公式F=P*Phi来计算得到，其中F是载荷阵，P为因子模式矩阵，Phi为因子关联矩阵。
通过自编函数 fsm ( )可以用于该乘法计算,自编函数的具体代码参考《R语言实战》一书。可以看到变量与因子间的相关系数。
将它们与正交旋转所得因子载荷阵相比，发现该载荷阵列的噪音较大，这是因为之前允许潜在因子相关。虽然斜交方法更为复杂，但模型将更加符合真实数据。
通过考虑以上三个矩阵，我们除了可以将不同变量重点归于某一因子外，还可以得到不同因子之间的相关系数。
通过图形显示
可通过factor.plot ( )函数来得到两因子图形，可从图中观测出不同初始变量在哪个因子上载荷更大。
factor.plot(fa.promax, labels=rownames(fa.promax$loadings))
还可通过fa.diagram ( )函数得到每个因子下的最大载荷，以及因子间的相关系数。
fa.diagram(fa.promax, simple=TRUE)  
3.5. 因子得分
EFA并不十分关注因子得分,想要得到因子得分，在提取时采用score=TRUE函数即可，还可以得到得分系数（标准化的回归权重），
在提取因子时得分系数将会体现在返回对象weights元素中。
fa.promax$weights
而且，这里与主成分分析不同，因子得分无法精确计算，只能估计得到。
原因在于，我们的因子数总比初始变量要少，通常因子得分的估计方法有：回归估计法，Bartlett估计法，Thomson估计法。
3.6. 评价
以各因子的方差贡献率为权，由各因子的线性组合得到综合评价指标函数。
F = (w1F1+w2F2+…+wmFm)/(w1+w2+…+wm )
此处wi为旋转前或旋转后因子的方差贡献率。
利用综合得分可以得到得分名次。
4. 其他
对因子分析非常有用的软件包，FactoMineR包不仅提供了PCA和EFA方法，还包含潜变量模型。
FAiR包使用遗传算法来估计因子分析模型，增强了模型参数估计能力，能够处理不等式的约束条件；
GPArotation包提供了许多因子旋转方法，
nFactors包，提供了用来判断因子数目方法。
先验知识的模型：先从一些先验知识开始，比如变量背后有几个因子、变量在因子上的载荷是怎样的、因子间的相关性如何，
然后通过收集数据检验这些先验知识。这种方法称作验证性因子分析（CFA）。做CFA的软件包：sem、openMx和lavaan等。
ltm包可以用来拟合测验和问卷中各项目的潜变量模型。
潜类别模型（潜在的因子被认为是类别型而非连续型）可通过FlexMix、lcmm、randomLCA和poLC包进行拟合。
lcda包可做潜类别判别分析，而lsa可做潜在语义分析----一种自然语言处理中的方法。ca包提供了可做简单和多重对应分析的函数。
R中还包含了众多的多维标度法（MDS）计算工具。MDS即可用发现解释相似性和可测对象间距离的潜在维度。
cmdscale（）函数可做经典的MDS
MASS包中的isoMDS（）函数可做非线性MDS
vagan包中则包含了两种MDS的函数
5. 总结
主成分分析和因子分析的步骤可归纳为：
1. 数据预处理:
PCA和EFA都是根据观测变量间的相关性来推导结果。
我们可以输入原始数据矩阵或相关系数矩阵列到principal()和fa（）函数中，若输入初始结果，相关系数矩阵将会被自动计算，在计算前请确保数据中没有缺失值；
2. 选择模型，是主成分分析还是因子分析：
判断是PCA（数据降维）还是EFA（发现潜在结构）更符合你的分析目标。若选择EFA方法时，还需要选择一种估计因子模型的方法（如最大似然估计）。
3. 判断要选择的主成分/因子数目
4. 选择主成分/因子
5. 旋转主成分/因子
6. 解释结果
7. 计算主成分/因子得分

