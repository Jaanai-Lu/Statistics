回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。
这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。
回归分析是建模和分析数据的重要工具。在这里，我们使用曲线/线来拟合这些数据点，在这种方式下，从曲线或线到数据点的距离差异最小。
我们为什么使用回归分析？
如上所述，回归分析估计了两个或多个变量之间的关系。下面，让我们举一个简单的例子来理解它：
比如说，在当前的经济条件下，你要估计一家公司的销售额增长情况。现在，你有公司最新的数据，这些数据显示出销售额增长大约是经济增长的2.5倍。
那么使用回归分析，我们就可以根据当前和过去的信息来预测未来公司的销售情况。
使用回归分析的好处良多。具体如下：
它表明自变量和因变量之间的显著关系；
它表明多个自变量对一个因变量的影响强度。
回归分析也允许我们去比较那些衡量不同尺度的变量之间的相互影响，如价格变动与促销活动数量之间联系。
这些有利于帮助市场研究人员，数据分析人员以及数据科学家排除并估计出一组最佳的变量，用来构建预测模型。
有各种各样的回归技术用于预测。这些技术主要有三个度量：
no of independent variables自变量的个数
type of dependent variable因变量的类型
shape of the regression line回归线的形状
我们将在下面的部分详细讨论它们。

01 Linear Regression线性回归
它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。
在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。
线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。
用一个方程式来表示它，即Y=a+b*X + e，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。
一元线性回归和多元线性回归的区别在于，多元线性回归有（>1）个自变量，而一元线性回归通常只有1个自变量。现在的问题是“我们如何得到一个最佳的拟合线呢？”。
如何获得最佳拟合线（a和b的值）？
这个问题可以使用最小二乘法轻松地完成。最小二乘法也是用于拟合回归线最常用的方法。
对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。
我们可以使用R-square指标来评估模型性能。
要点：
自变量与因变量之间必须有线性关系
多元回归存在多重共线性，自相关性和异方差性。
线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。
结果就是系数估计值不稳定，在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

02 Logistic Regression逻辑回归
逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。
当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示。
odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk
上述式子中，p为某个特征的概率。你应该会问这样一个问题：“我们为什么要在公式中使用对数log呢？”。
因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。
在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。
要点：
它广泛地用于分类问题。
逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。
为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。
它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。
自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。
如果因变量的值是定序变量，则称它为序逻辑回归。
如果因变量是多类的话，则称它为多元逻辑回归。

03 Polynomial Regression多项式回归
对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：
y=a+b*x^2
在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。
重点：
虽然可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。
你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。

04 Stepwise Regression逐步回归
在处理多个自变量时，我们可以使用这种形式的回归。
在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。
这一壮举是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。
逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。下面列出了一些最常用的逐步回归方法：
标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。
向前选择法从模型中最显著的预测开始，然后为每一步添加变量。
向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显著性的变量。
这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。

05 Ridge Regression岭回归
岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。
在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。
岭回归通过给回归估计上增加一个偏差度，来降低标准误差（有偏估计回归）。
上面，我们看到了线性回归方程：y=a+ b*x
这个方程也有一个误差项。完整的方程是：
y=a+b*x+e (error term)
[error term is the value needed to correct for a prediction error between the observed and predicted value]
=> y=a+y= a+ b1x1+ b2x2+....+e, for multiple independent variables.
在一个线性方程中，预测误差可以分解为2个子分量。一个是偏差，一个是方差。
预测错误可能会由这两个分量或者这两个中的任何一个造成。在这里，我们将讨论由方差所造成的有关误差。
岭回归通过收缩参数λ（lambda）解决多重共线性问题。
公式中有两个组成部分。第一个是最小二乘项，另一个是β2（β-平方）的λ倍，其中β是相关系数。为了收缩参数把它添加到最小二乘项中以得到一个非常低的方差。
要点：
除常数项以外，这种回归的假设与最小二乘回归类似；它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能。
这是一个正则化方法，并且使用的是L2正则化。代价是损失信息，降低精度。

06 Lasso Regression套索回归
它类似于岭回归，Lasso（Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。
此外，它能够减少变化程度并提高线性回归模型的精度。
Lasso 回归与Ridge回归有一点不同，它使用的惩罚函数是绝对值，而不是平方。
这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。
使用惩罚值越大，进一步估计会使得缩小值趋近于零。
这将导致我们要从给定的n个变量中选择变量。
要点：
除常数项以外，这种回归的假设与最小二乘回归类似；它收缩系数接近零（等于零），确实有助于特征选择；
这是一个正则化方法，使用的是L1正则化；
如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。

07 ElasticNet回归
ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。
当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。
Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。
要点：
在高度相关变量的情况下，它会产生群体效应；
选择变量的数目没有限制；
它可以承受双重收缩。

08稳健回归 (robust regression)
数据中有离群值，做线性回归对结果影响大吗？答案是肯定的。
我们可以通过Cook’s距离来识别明显的离群值，剔除后再进行线性回归。
但如果在离群值不明显、数量较多、研究者无充分理由认为可能的离群值有错误的情况下，直接剔除离群值可能不太合适。
那么，还有其他办法可以处理离群值的问题吗？
稳健回归通过对数据中各样本赋予不同的权重来考虑离群值对回归方程的影响，可以作为最小二乘法 (传统线性回归) 的替代。
当然，通过稳健回归结果中不同样本的权重，我们可以识别离群值，或者是找出重要的样本点。
下面我们用R语言模拟两个样本量为100的具有线性相关关系的变量x和y，然后在此基础上增加3个离群值，以期观察离群值对回归方程的影响。
#设定随机种子
set.seed(2019)
#生成自变量x与因变量y
x=rnorm(100)
y=x+1+rnorm(n=100,mean = 0, sd = 0.5)
#增加3个离群值
x=c(x,-3+rnorm(3,sd = 0.3))
y=c(y,rep(2,3))
#作散点图，并标注离群点
plot(x,y)
points(x[101:103],y[101:103],col=2,pch=16)
#未剔除离群点做线性回归，并作直线
ols=lm(y~x)
abline(ols)
#剔除离群点后做线性回归，并作直线
ols0=lm(y[1:100]~x[1:100])
abline(ols0,col=4,lwd=8)
#大家可自行查看离群点
#plot(ols)


下图是运行结果，左上角的红点为3个离群点，黑色线为未剔除离群点所拟合的直线，而蓝色线为剔除离群点后拟合的直线。我们可以看到离群点对模型的影响还是很大的。



下图是Cook’s距离，红色虚线为界值，编号为101、102、103的样本点都被准确识别出来。接下来，我们拟合稳健回归以考虑离群值的影响。



  稳健回归  

稳健回归考虑离群值的关键在于为各样本赋予不同的权重，计算权重的函数有很多，包括andrews、bisquare、cauchy、fair、huber、logistic、talwar、welsch等，其中bisquare是最常用的一种，也是Matlab软件中稳健回归的默认权重函数。接着上面的代码，我们在未剔除离群点的数据中拟合稳健回归。

#加载R包
library(MASS)
#未剔除离群点做稳健回归，并作直线
bisquare=rlm(y ~ x, psi = psi.bisquare)
abline(bisquare,col="white",lty=2,lwd=2)
#查看各样本的权重
bisquare$w


下图是运行结果，白色虚线正是稳健回归的结果，基本与蓝色线重叠，可以说明稳健回归可以很好的处理带有离群值的情况。



下图是各样本的权重，可以看出我们人为添加的3个离群点被赋予了0的权重，而其他100个点都基本在0.9左右，而在传统的线性回归中，所有样本点的权重都为1

除了这8个最常用的回归技术，也还有其他模型，如Bayesian和Ecological回归。
如何正确选择回归模型？
在多类回归模型中，基于自变量和因变量的类型，数据的维数以及数据的其它基本特征的情况下，选择最合适的技术非常重要。
以下是选择正确的回归模型的关键因素：
数据探索是构建预测模型的必然组成部分。在选择合适的模型时，比如识别变量的关系和影响时，它应该是首选的一步。
比较适合于不同模型的优点，我们可以分析不同的指标参数，如统计意义的参数，R-square，Adjusted R-square，AIC，BIC以及误差项，
另一个是Mallows' Cp准则。这个主要是通过将模型与所有可能的子模型进行对比（或谨慎选择他们），检查在你的模型中可能出现的偏差。
交叉验证是评估预测模型最好的方法。在这里，将你的数据集分成两份（一份做训练和一份做验证）。使用观测值和预测值之间的一个简单均方差来衡量你的预测精度。
如果数据集是多个混合变量，那么就不应该选择自动模型选择方法，因为你应该不想在同一时间把所有变量放在同一个模型中。
它也将取决于你的目的。可能会出现这样的情况，一个不太强大的模型与具有高度统计学意义的模型相比，更易于实现。
回归正则化方法（Lasso，Ridge和ElasticNet）在高维和数据集变量之间多重共线性情况下运行良好。
